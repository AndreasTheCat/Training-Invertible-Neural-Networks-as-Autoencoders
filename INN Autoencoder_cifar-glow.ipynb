{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from functionalities import dataloader as dl\n",
    "from functionalities import tracker as tk\n",
    "from architecture import INN as inn\n",
    "from functionalities import CIFAR_coder_loss as cl\n",
    "from functionalities import trainer as tr\n",
    "from functionalities import filemanager as fm\n",
    "from functionalities import plot as pl\n",
    "from functionalities import gpu "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epoch = 15\n",
    "batch_size = 128\n",
    "latent_dim_lst = [2 ** x for x in range(11)]\n",
    "#latent_dim = 400\n",
    "number_dev = 0\n",
    "lr_init = 1e-3\n",
    "l2_reg  = 1e-6\n",
    "milestones = [10, 15]\n",
    "modelname = 'cifar_INN_glow_com_bottleneck'\n",
    "get_model = inn.cifar_inn_com\n",
    "\n",
    "device = gpu.get_device(number_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset, classes = dl.load_cifar()\n",
    "trainloader, validloader, testloader = dl.make_dataloaders(trainset, testset, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tr.train_bottleneck(num_epoch, get_model, 'l1', modelname, milestones, latent_dim_lst, trainloader, None, \n",
    "                            testloader, a_distr=0, a_disen=0, lr_init=lr_init, l2_reg=l2_reg, device=device, save_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Reconstruction and Difference Images Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for lat_dim in latent_dim_lst:\n",
    "    print(\"Latent Dimension: \", lat_dim)\n",
    "    model = fm.load_model('{}_{}_{}'.format(modelname, lat_dim, num_epoch), \"{}_bottleneck\".format(modelname))\n",
    "    pl.plot_diff(model.to('cuda'), trainloader, lat_dim, device, 100, 10, filename='com_INN_cifar_{}'.format(lat_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Recontruction Loss against Bottleneck Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, l1_rec_test, _, _, _ = fm.load_variable('bottleneck_test_loss_{}'.format(modelname), modelname)\n",
    "_, l1_rec_train, _, _, _ = fm.load_variable('bottleneck_train_loss_{}'.format(modelname), modelname)\n",
    "\n",
    "pl.plot(latent_dim_lst, [l1_rec_train, l1_rec_test], 'bottleneck size', 'loss', ['train', 'test'], 'Test Reconstruction Loss History', '{}_bottleneck_History'.format(modelname)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "def get_loss(loader, model, criterion, latent_dim, device='cpu'):\n",
    "    \"\"\"\n",
    "    Compute the loss of a model on a train, test or evalutation set wrapped by a loader.\n",
    "\n",
    "    :param loader: loader that wraps the train, test or evaluation set\n",
    "    :param model: model that should be tested\n",
    "    :param criterion: the criterion to compute the loss\n",
    "    :param latent_dim: dimension of the latent space\n",
    "    :param tracker: tracker for values during training\n",
    "    :param device: device on which to do the computation (CPU or CUDA). Please use get_device() function to get the\n",
    "    device, if using multiple GPU's. Default: cpu\n",
    "    :return: losses\n",
    "    \"\"\"\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    losses = np.zeros(5, dtype=np.double)\n",
    "\n",
    "    #tracker.reset()\n",
    "\n",
    "    for i, data in enumerate(tqdm(loader), 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            lat_img = model(inputs)\n",
    "            lat_shape = lat_img.shape\n",
    "            lat_img = lat_img.view(lat_img.size(0), -1)\n",
    "\n",
    "            lat_img_mod = torch.cat([lat_img[:, :latent_dim], lat_img.new_zeros((lat_img[:, latent_dim:]).shape)], dim=1)\n",
    "            lat_img_mod = lat_img_mod.view(lat_shape)\n",
    "\n",
    "            output = model(lat_img_mod, rev=True)\n",
    "\n",
    "            batch_loss = criterion(inputs, lat_img, output)\n",
    "\n",
    "            for i in range(len(batch_loss)):\n",
    "                losses[i] += batch_loss[i].item() * 100\n",
    "\n",
    "     #       tracker.update(lat_img)\n",
    "\n",
    "    losses /= len(loader)\n",
    "    return losses\n",
    "\n",
    "\n",
    "def get_loss_bottleneck(loader, modelname, subdir, latent_dim_lst, num_epoch, device, a_distr, a_rec, a_spar, a_disen):\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    total_loss = []\n",
    "    rec_loss = []\n",
    "    dist_loss = []\n",
    "    spar_loss = []\n",
    "    disen_loss = []\n",
    "\n",
    "    for i in latent_dim_lst:\n",
    "        print('bottleneck dimension: {}'.format(i))\n",
    "        model = fm.load_model('{}_{}_{}'.format(modelname, i, num_epoch), subdir).to(device)\n",
    "        criterion = cl.CIFAR_coder_loss(a_distr=a_distr, a_rec=a_rec, a_spar=a_spar, a_disen=a_disen, latent_dim=i, loss_type='l1', device=device)\n",
    "        losses = get_loss(loader, model, criterion, i, device)\n",
    "        total_loss.append(losses[0])\n",
    "        rec_loss.append(losses[1])\n",
    "        dist_loss.append(losses[2])\n",
    "        spar_loss.append(losses[3])\n",
    "        disen_loss.append(losses[4])\n",
    "\n",
    "    return total_loss, rec_loss, dist_loss, spar_loss, disen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = latent_dim_lst\n",
    "_, y, _, _, _ = get_loss_bottleneck(testloader, modelname, modelname + '_bottleneck', latent_dim_lst, num_epoch, device, 0, 1, 1, 0)\n",
    "pl.plot(x, y, 'latent dimension', 'loss', 'l1', 'Test Reconstruction Loss History', '{}_bottleneck_History'.format(modelname)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
